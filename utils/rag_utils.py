# rag_utils.py

import streamlit as st
from io import BytesIO
import tempfile
import os
import json
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# LangChain imports
from langchain_community.document_loaders import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain.chains import RetrievalQA

# --- 1. RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬) ---
def setup_rag_database_from_files(uploaded_files):
    """
    ì—…ë¡œë“œëœ CSV íŒŒì¼ë“¤ë¡œë¶€í„° RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
    """
    print(f"1ë‹¨ê³„: {len(uploaded_files)}ê°œ ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë”©...")
    all_docs = []
    
    for uploaded_file in uploaded_files:
        filename = uploaded_file.name
        print(f"   -> {filename} ì²˜ë¦¬ ì¤‘...")
        
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
            try:
                df = pd.read_csv(uploaded_file, encoding='utf-8', dtype=str)
            except UnicodeDecodeError:
                uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
                df = pd.read_csv(uploaded_file, encoding='cp949', dtype=str)
                print(f"   -> '{filename}'ì€ cp949 ì¸ì½”ë”©ìœ¼ë¡œ ë¡œë“œ")

            for _, row in df.iterrows():
                content = str(row.get("ë³´ê³ ì„œ ë‚´ìš©", ""))
                raw_date = str(row.get("ë‚ ì§œ", "")).rstrip()
                try:
                    dt = pd.to_datetime(raw_date, errors='coerce')
                    year_month = dt.strftime('%Y%m') if pd.notna(dt) else (raw_date[:6] if raw_date else "")
                except:
                    year_month = raw_date[:6] if raw_date else ""
                metadata = { "ë…„ì›”": year_month, "ê´‘ë¬¼": str(row.get("ê´‘ë¬¼", "")), "source": filename }
                doc = Document(page_content=content, metadata=metadata)
                all_docs.append(doc)
            print(f"   -> {filename} ë¡œë“œ ì™„ë£Œ ({len(df)}ê°œ í–‰)")
        except Exception as e:
            print(f"   - ì˜¤ë¥˜: {filename} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ - {e}")
    
    if not all_docs:
        print("ê²½ê³ : ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
    splits = text_splitter.split_documents(all_docs)
    print("í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥...")
    embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
    print("âœ… ì„±ê³µ: RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ.")
    return vectorstore, embedding_model

# --- 1-1. ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë°©ì‹ (í˜¸í™˜ì„± ìœ ì§€) ---
def setup_rag_database(data_directory: str):
    """
    CSV íŒŒì¼ë“¤ë¡œë¶€í„° RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (ê¸°ì¡´ ë°©ì‹)
    """
    print(f"1ë‹¨ê³„: '{data_directory}' ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„° ë¡œë”©...")
    all_docs = []
    if not os.path.exists(data_directory):
        print(f"ê²½ê³ : '{data_directory}' ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    for filename in os.listdir(data_directory):
        if filename.endswith(".csv"):
            file_path = os.path.join(data_directory, filename)
            try:
                try:
                    df = pd.read_csv(file_path, encoding='utf-8', dtype=str)
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='cp949', dtype=str)
                    print(f"   -> '{filename}'ì€ cp949 ì¸ì½”ë”©ìœ¼ë¡œ ë¡œë“œ")

                for _, row in df.iterrows():
                    content = str(row.get("ë³´ê³ ì„œ ë‚´ìš©", ""))
                    raw_date = str(row.get("ë‚ ì§œ", "")).rstrip()
                    try:
                        dt = pd.to_datetime(raw_date, errors='coerce')
                        year_month = dt.strftime('%Y%m') if pd.notna(dt) else (raw_date[:6] if raw_date else "")
                    except:
                        year_month = raw_date[:6] if raw_date else ""
                    metadata = { "ë…„ì›”": year_month, "ê´‘ë¬¼": str(row.get("ê´‘ë¬¼", "")), "source": filename }
                    doc = Document(page_content=content, metadata=metadata)
                    all_docs.append(doc)
                print(f"   -> {filename} ë¡œë“œ ì™„ë£Œ.")
            except Exception as e:
                print(f"   - ì˜¤ë¥˜: {filename} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ - {e}")
    
    if not all_docs:
        print("ê²½ê³ : ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
    splits = text_splitter.split_documents(all_docs)
    print("í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥...")
    embedding_model = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
    print("âœ… ì„±ê³µ: RAG ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ.")
    return vectorstore, embedding_model

# 1) ê¸°ì¡´ RAG ì²´ì¸ ì´ˆê¸°í™” í•¨ìˆ˜ (Streamlitìš© - ë‹¨ì¼ CSV íŒŒì¼ ì²˜ë¦¬)
@st.cache_resource(show_spinner=False)
def init_rag_chain(csv_bytes: bytes):
    """
    CSV íŒŒì¼(bytes) â†’ Document ë¡œë”© â†’
    HuggingFace ì„ë² ë”© â†’ Chroma DB ìƒì„± â†’
    RetrievalQA ì²´ì¸ ë°˜í™˜
    """
    # 1. CSV â†’ Document
    with tempfile.NamedTemporaryFile(mode='wb', suffix='.csv', delete=False) as tmp_file:
        tmp_file.write(csv_bytes)
        tmp_file_path = tmp_file.name
    
    try:
        loader = CSVLoader(file_path=tmp_file_path, encoding="utf-8")
        docs = loader.load()
    except:
        # pandasë¡œ ì§ì ‘ ì²˜ë¦¬
        df = pd.read_csv(tmp_file_path, encoding='utf-8')
        docs = [Document(page_content=str(row.to_dict()), metadata={"source": f"row_{i}"}) 
                for i, row in df.iterrows()]
    finally:
        os.unlink(tmp_file_path)

    # 2. í…ìŠ¤íŠ¸ ë¶„í•  ë° ì„ë² ë”©
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)
    
    embeddings = HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)

    # 3. RetrievalQA ì²´ì¸ ìƒì„±
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectordb.as_retriever(),
        return_source_documents=False
    )

    return qa

# --- 2. ì•ˆì „í•œ JSON íŒŒì‹±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ ---
def safe_json_loads(response_str: str, default_value=None):
    if default_value is None:
        default_value = {}
    try:
        if '```json' in response_str:
            response_str = response_str.split('```json')[1].split('```')[0]
        return json.loads(response_str.strip())
    except (json.JSONDecodeError, IndexError):
        return default_value

# --- 3. ê³ ë„í™”ëœ ë¶„ì„ ì²´ì¸ (ë…¸íŠ¸ë¶ì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜) ---
def create_improved_analysis_chain():
    """'ìƒê°ì˜ ì‚¬ìŠ¬', 'ìê¸° ë¹„íŒ', 'ê°•ë„' í‰ê°€ë¥¼ í¬í•¨í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì¸ ë¶„ì„ ì²´ì¸"""
    prompt_template = ChatPromptTemplate.from_template(
        """
[AI ì‘ì—… ì§€ì¹¨ - ìˆ˜ì…ì ë¦¬ìŠ¤í¬ ë¶„ì„ (ê³ ë„í™” ë²„ì „)]

ë‹¹ì‹ ì€ 'ë¹„ìš©'ê³¼ 'ê³µê¸‰ ì•ˆì •ì„±' ê´€ì ì—ì„œë§Œ í‰ê°€í•˜ëŠ” ëŒ€í•œë¯¼êµ­ ì›ìì¬ êµ¬ë§¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ê°€ê²© ìƒìŠ¹ì´ë‚˜ ê³µê¸‰ë§ ë¶ˆì•ˆì •ì€ ì›ì¸ê³¼ ê´€ê³„ì—†ì´ ë¶€ì •ì  ì‹ í˜¸ì…ë‹ˆë‹¤.

[ë¶„ì„ ì ˆì°¨ (4ë‹¨ê³„)]

### 1ë‹¨ê³„: ì‚¬ì‹¤ ì‹ë³„ (Fact Identification)
- ë¬¸ì¥ì—ì„œ {target_item} ìˆ˜ì…ê³¼ ê´€ë ¨ëœ ê°ê´€ì ì¸ ì‚¬ì‹¤(event)ì„ ë¨¼ì € ì‹ë³„í•©ë‹ˆë‹¤.

### 2ë‹¨ê³„: ì˜í–¥ ë¶„ì„ (Impact Analysis - Chain of Thought)
- ê° ì‚¬ì‹¤ì´ ëŒ€í•œë¯¼êµ­ì˜ {target_item} ìˆ˜ì…ì— ì–´ë–¤ ì—°ì‡„ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•˜ì„¸ìš”.
- (ì˜ˆ: "Aì‚¬ íŒŒì—…" -> "ìƒì‚° ì°¨ì§ˆ ë°œìƒ" -> "ì‹œì¥ ê³µê¸‰ëŸ‰ ê°ì†Œ" -> "ìˆ˜ì… ê°€ê²© ìƒìŠ¹ ì••ë ¥")
- ì´ ì¶”ë¡  ê³¼ì •ì„ 'reason'ì— ëª…í™•íˆ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤.

### 3ë‹¨ê³„: ì´ˆê¸° ë¶„ë¥˜ (Initial Classification)
- 'ì˜í–¥ ë¶„ì„' ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ì‚¬ê±´ì´ ìˆ˜ì…ìì—ê²Œ [Positive], [Negative], [Neutral] ì¤‘ ë¬´ì—‡ì¸ì§€, ê·¸ë¦¬ê³  ê·¸ ì˜í–¥ì˜ ê°•ë„ë¥¼ [High, Medium, Low] ì¤‘ì—ì„œ ì ì •ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.

### 4ë‹¨ê³„: ìê¸° ë¹„íŒ ë° ìµœì¢… ê²°ì • (Self-Critique & Final Decision)
- ë‚´ë¦° ì ì • íŒë‹¨ì´ [ìµœì¢… íŒë‹¨ ì›ì¹™]ì— ë¶€í•©í•˜ëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ê²€í† í•˜ì„¸ìš”.
- (ì˜ˆ: "ì´ ì‚¬ê±´ì€ ê²½ê¸° íšŒë³µ ì‹ í˜¸ë¡œ ë³¼ ìˆ˜ë„ ìˆì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ 'ê°€ê²© ìƒìŠ¹'ì„ ìœ ë°œí–ˆìœ¼ë¯€ë¡œ, ìˆ˜ì…ì ê´€ì ì—ì„œëŠ” ìµœì¢…ì ìœ¼ë¡œ [Negative]ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ì›ì¹™ì— ë§ë‹¤.")
- ì´ ê²€í†  ê³¼ì •ì„ ê±°ì³ ìµœì¢… ë¶„ë¥˜(classification)ì™€ ê°•ë„(intensity)ë¥¼ í™•ì •í•©ë‹ˆë‹¤.

---

[ìµœì¢… íŒë‹¨ ì›ì¹™]
- ğŸš¨ **ê°€ê²© ë³€ë™ ìµœìš°ì„  ì›ì¹™**: 'ê°€ê²© ìƒìŠ¹' ë˜ëŠ” 'ìƒìŠ¹ ì••ë ¥'ì´ ì–¸ê¸‰ë˜ë©´, ë‹¤ë¥¸ ê¸ì •ì  ë§¥ë½(ì˜ˆ: ìˆ˜ìš” ì¦ê°€)ì´ ìˆë”ë¼ë„ ë¬´ì¡°ê±´ [Negative]ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ 'ê°€ê²© í•˜ë½'ì€ [Positive]ì…ë‹ˆë‹¤.
- **ê³µê¸‰ë§ ë¦¬ìŠ¤í¬ ìš°ì„  ì›ì¹™**: ê³µê¸‰ ìì²´ë¥¼ ìœ„í˜‘í•˜ëŠ” 'ì „ìŸ', 'ìˆ˜ì¶œ ì „ë©´ ê¸ˆì§€' ë“±ì€ ê°€ê²©ê³¼ ë¬´ê´€í•˜ê²Œ [Negative]ì´ë©°, ê°•ë„ëŠ” [High]ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
- **ê³¼ì‰ ì¶”ë¡  ê¸ˆì§€**: ë¬¸ì¥ì— ëª…ì‹œëœ ì‚¬ì‹¤ì—ë§Œ ê·¼ê±°í•©ë‹ˆë‹¤.

---

[ë¶„ì„ ëŒ€ìƒ]
- Target Item: {target_item}
- Context: {context}

---

# ì¶œë ¥ JSON í˜•ì‹:

{{
    "analysis": [
        {{
            "sentence": "ë¶„ì„ ëŒ€ìƒ ë¬¸ì¥ ì›ë³¸",
            "classification": "Positive, Negative, Neutral ì¤‘ í•˜ë‚˜",
            "intensity": "High, Medium, Low ì¤‘ í•˜ë‚˜",
            "reason": "ìœ„ ë¶„ì„ ì ˆì°¨ 2ë‹¨ê³„ì™€ 4ë‹¨ê³„ì— ë”°ë¥¸ ìƒì„¸í•œ íŒë‹¨ ê³¼ì • ì„œìˆ "
        }}
    ],
    "overall_summary": "ì´ë²ˆ ë‹¬ì˜ ê¸ì •/ë¶€ì • ìš”ì¸ë“¤ì„ ì¢…í•©í•˜ì—¬ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."
}}
"""
    )
    # ê³ í’ˆì§ˆì˜ íŒë‹¨ì„ ìœ„í•´ ë” ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite-preview-06-17", temperature=0)
    return prompt_template | llm | StrOutputParser()

# --- 4. LLM ë¬¸ì¥ ë¶„ë¦¬ê¸° (ë…¸íŠ¸ë¶ì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜) ---
def get_sentences_from_text_with_llm(text_block: str, llm: ChatGoogleGenerativeAI) -> list[str]:
    print("   -> LLMì„ ì´ìš©í•´ ë¬¸ì¥ ë¶„ë¦¬ ì‘ì—… ì‹œì‘...")
    segment_prompt = ChatPromptTemplate.from_template(
    """
 # ì—­í• :
        ë‹¹ì‹ ì€ ê¸´ ë³´ê³ ì„œì—ì„œ 'ë‹¨ì¼ ì‚¬ê±´(a single, atomic event)'ì„ ì¶”ì¶œí•˜ì—¬, ê°ê°ì˜ ë…ë¦½ì ì¸ ì˜ë¯¸ë¥¼ ê°€ì§„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ë¬¸ë§¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        # í•µì‹¬ ì§€ì‹œì‚¬í•­:
        1.  ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ì™„ì „í•œ í•˜ë‚˜ì˜ ì•„ì´ë””ì–´ë¥¼ ë‹´ê³  ìˆëŠ” ë‹¨ìœ„ë¡œ ë¬¶ê±°ë‚˜ ë‚˜ëˆ ì£¼ì„¸ìš”.

        2.  **[ë¶„ë¦¬ ì›ì¹™]** í•˜ë‚˜ì˜ ë¬¸ì¥ ì•ˆì— ì ‘ì†ì‚¬(~í–ˆìœ¼ë©°, ~í•˜ê³  ë“±)ë¡œ ì—°ê²°ëœ ì—¬ëŸ¬ ì‚¬ê±´ì´ ìˆë‹¤ë©´, ê°ê°ì˜ **ë…ë¦½ì ì¸ ì‚¬ê±´ìœ¼ë¡œ ë°˜ë“œì‹œ ë¶„ë¦¬**í•´ì•¼ í•©ë‹ˆë‹¤.
            - ì˜ˆì‹œ: "ì¤‘êµ­ì€ Aë¥¼ í–ˆê³ , ë¯¸êµ­ì€ Bë¥¼ í–ˆë‹¤." -> "ì¤‘êµ­ì€ Aë¥¼ í–ˆë‹¤.", "ë¯¸êµ­ì€ Bë¥¼ í–ˆë‹¤."

        3.  ğŸš¨ **[ë§¤ìš° ì¤‘ìš” - ë³‘í•© ì›ì¹™]** ì´ì™€ ë°˜ëŒ€ë¡œ, **í•˜ë‚˜ì˜ ì‚¬ê±´ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë…¼ë¦¬ì ìœ¼ë¡œ ê°•í•˜ê²Œ ì—°ê²°ëœ ì—¬ëŸ¬ ë¬¸ì¥ì€ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ í•©ì³ì•¼ í•©ë‹ˆë‹¤.**
            - **ì›ì¸ê³¼ ê²°ê³¼, ì£¼ì¥ê³¼ ê·¼ê±°, í˜„ìƒê³¼ ë¶€ì—° ì„¤ëª…, ëª©ì ê³¼ ìˆ˜ë‹¨** ë“±ì€ ë¶„ë¦¬í•´ì„œëŠ” ì•ˆ ë˜ëŠ” ê°•ë ¥í•œ ì—°ê²° ê´€ê³„ì…ë‹ˆë‹¤.

        4.  ê° ë‹¨ìœ„ëŠ” ê·¸ ìì²´ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ë  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

        5.  ë‹µë³€ì€ ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì˜¤ì§ JSON í˜•ì‹ì˜ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

        # ì¢‹ì€ 'ë³‘í•©' ì˜ˆì‹œ (ì—¬ëŸ¬ ë¬¸ì¥ì„ í•˜ë‚˜ë¡œ í•©ì³ì•¼ í•˜ëŠ” ê²½ìš°):
        - ì›ë³¸: "Sherrittç¤¾ëŠ” ë¹„ìš©ì ˆê° ì¡°ì¹˜ì— ì°©ìˆ˜í–ˆìŠµë‹ˆë‹¤. ì¡°ì§ ê°„ì†Œí™”ë¥¼ í†µí•œ ì¸ë ¥ ê°ì¶• ë° ë¹„ìš© ì ˆê° í”„ë¡œê·¸ë¨ì„ ì¶”ì§„í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        - ì´ìƒì ì¸ ì¶œë ¥: ["Sherrittì‚¬ëŠ” ë¹„ìš© ì ˆê° ì¡°ì¹˜ì˜ ì¼í™˜ìœ¼ë¡œ, ì¡°ì§ ê°„ì†Œí™”ë¥¼ í†µí•œ ì¸ë ¥ ê°ì¶• í”„ë¡œê·¸ë¨ì„ ì¶”ì§„í•˜ê³  ìˆìŠµë‹ˆë‹¤."]

        # ğŸš¨ ì¢‹ì€ 'ë¶„ë¦¬' ì˜ˆì‹œ (í•˜ë‚˜ì˜ ë¬¸ì¥ì„ ì—¬ëŸ¬ ê°œë¡œ ë‚˜ëˆ ì•¼ í•˜ëŠ” ê²½ìš°):
        - ì›ë³¸: "ì¤‘êµ­ ìƒë¬´ë¶€ëŠ” ë¯¸êµ­ì˜ ì¸í”Œë ˆì´ì…˜ ê°ì¶•ë²•(IRA) ë³´ì¡°ê¸ˆ ì§€ê¸‰ ëŒ€ìƒì—ì„œ ì¤‘êµ­ì‚° ì „ê¸°ì°¨ë¥¼ ì œì™¸í•œ ê²ƒì— ëŒ€í•´ WTOì— ì œì†Œí–ˆìœ¼ë©°, ì¸ë„ë„¤ì‹œì•„ Nickel Industriesì‚¬ì˜ ë‹ˆì¼ˆ ìƒì‚°ëŸ‰ì€ ì „ë…„ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
        - ì´ìƒì ì¸ ì¶œë ¥: [
            "ì¤‘êµ­ ìƒë¬´ë¶€ëŠ” ë¯¸êµ­ IRAì˜ ì „ê¸°ì°¨ ë³´ì¡°ê¸ˆ ì •ì±…ì— ëŒ€í•´ WTOì— ì œì†Œí–ˆìŠµë‹ˆë‹¤.",
            "ì¸ë„ë„¤ì‹œì•„ Nickel Industriesì‚¬ì˜ ë‹ˆì¼ˆ ìƒì‚°ëŸ‰ì´ ì „ë…„ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
          ]

        # ì´ì œ ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•´ì£¼ì„¸ìš”:
        {text}
    """
    )
    segment_chain = segment_prompt | llm | StrOutputParser()
    response_str = segment_chain.invoke({"text": text_block})
    sentences = safe_json_loads(response_str, default_value=[]) if isinstance(response_str, str) else []

    if sentences:
        print(f"   -> ì„±ê³µ: {len(sentences)}ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬ ì™„ë£Œ.")
        return sentences
    else:
        print("   -> ë¬¸ì¥ ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í†µì§¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return [text_block]

# 2) ê¸°ì¡´ ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
def calculate_risk_score(answer: str) -> int:
    """
    AI ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ì ìˆ˜ ì‚°ì¶œ ì˜ˆì‹œ.
    ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.
    """
    score = 50
    text = answer.lower()

    if "ë†’ìŒ" in text or "ìœ„í—˜" in text:
        score += 30
    if "ì¤‘ê°„" in text:
        score += 15
    if "ë‚®ìŒ" in text or "ì•ˆì •" in text:
        score -= 10

    # 0~100 ë²”ìœ„ë¡œ í´ë¨í•‘
    return max(0, min(100, score))

# --- 5. ì›”ë³„ AI ì§€ìˆ˜ ìƒì„± (ë…¸íŠ¸ë¶ì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜) ---
def create_monthly_ai_index_with_intensity(start_year: int, start_month: int, end_year: int, end_month: int, vectorstore, analysis_chain, target_mineral="ë‹ˆì¼ˆ"):
    """
    ì›”ë³„ AI ì§€ìˆ˜ ìƒì„± - ê°•ë„ ê°€ì¤‘ì¹˜ ì ìš©
    """
    print(f"\nìµœì¢… ë¶„ì„ ë‹¨ê³„: {start_year}ë…„ {start_month}ì›”ë¶€í„° ì›”ë³„ ìˆ˜ì… ì‹¬ë¦¬ ì§€ìˆ˜(ê°•ë„ ê°€ì¤‘) ìƒì„± ì‹œì‘...")
    monthly_results = []
    llm_for_splitter = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite-preview-06-17", temperature=0)

    date_range = pd.date_range(start=f'{start_year}-{start_month}-01', end=f'{end_year}-{end_month}-01', freq='MS')

    for month_start_date in date_range:
        current_month_str_ym = month_start_date.strftime('%Y%m')
        current_month_str_kor = month_start_date.strftime('%Yë…„ %mì›”')
        print(f"--- {current_month_str_kor} ë¶„ì„ ì¤‘ ---")

        filter_dict = {"$and": [{"ê´‘ë¬¼": {"$eq": target_mineral}}, {"ë…„ì›”": {"$eq": current_month_str_ym}}]}
        retrieved_texts = vectorstore.get(where=filter_dict, include=["documents"]).get('documents', [])

        if not retrieved_texts:
            print("   -> í•´ë‹¹ ê¸°ê°„ì— ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            monthly_results.append({ "date": month_start_date, "nsi_score": np.nan, "summary": "ë°ì´í„° ì—†ìŒ", "analysis_results": [] })
            continue

        raw_context_str = "\n\n".join(retrieved_texts)
        all_sentences = get_sentences_from_text_with_llm(raw_context_str, llm_for_splitter)

        batch_size = 20
        all_analysis_results = []
        print(f"   -> ì´ {len(all_sentences)}ê°œ ë¬¸ì¥ì„ {batch_size}ê°œì”© ë‚˜ëˆ„ì–´ ê³ ë„í™”ëœ ë¶„ì„ ì‹¤í–‰...")

        for i in range(0, len(all_sentences), batch_size):
            batch_sentences = all_sentences[i:i + batch_size]
            final_context = "\n".join([f"- {s}" for s in batch_sentences])
            print(f"     -> ë°°ì¹˜ {i//batch_size + 1} ë¶„ì„ ì¤‘ ({len(batch_sentences)}ê°œ ë¬¸ì¥)")

            try:
                response_str = analysis_chain.invoke({"context": final_context, "target_item": target_mineral})
                response_json = safe_json_loads(response_str)

                # ìƒˆë¡œìš´ JSON êµ¬ì¡°ì— ë§ì¶° ê²°ê³¼ íŒŒì‹±
                for item in response_json.get("analysis", []):
                    all_analysis_results.append({
                        "text": item.get("sentence"),
                        "classification": item.get("classification"),
                        "intensity": item.get("intensity", "Low"), # intensityê°€ ì—†ìœ¼ë©´ Low ê¸°ë³¸ê°’
                        "reason": item.get("reason", "N/A")
                    })
                time.sleep(1)
            except Exception as e:
                print(f"     -> ë°°ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # --- ê°•ë„(Intensity)ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•œ NSI ì ìˆ˜ ê³„ì‚° ---
        intensity_map = {"High": 3, "Medium": 2, "Low": 1}
        p_weighted_sum = 0
        n_weighted_sum = 0

        for item in all_analysis_results:
            classification = item.get("classification")
            intensity_str = item.get("intensity", "Low")
            weight = intensity_map.get(intensity_str, 1)

            if classification == "Positive":
                p_weighted_sum += weight
            elif classification == "Negative":
                n_weighted_sum += weight

        total_analyzed_count = sum(1 for item in all_analysis_results if item.get("classification") in ["Positive", "Negative"])
        k = 0 # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  ì ìˆ˜ ë³€ë™ì„ ì™„ë§Œí•˜ê²Œ í•˜ëŠ” ìƒìˆ˜

        if total_analyzed_count > 0:
            nsi_score = (p_weighted_sum - n_weighted_sum) / (p_weighted_sum + n_weighted_sum + k)
        else:
            nsi_score = 0.0

        final_summary = f"ì´ {total_analyzed_count}ê°œ ìš”ì¸ ë¶„ì„ (ê°•ë„ ê°€ì¤‘ì¹˜ ì ìš©): ê¸ì • ê°€ì¤‘í•© {p_weighted_sum}, ë¶€ì • ê°€ì¤‘í•© {n_weighted_sum}."

        print(f"   -> ìµœì¢… ê¸ì •/ë¶€ì • ê°€ì¤‘í•©: {p_weighted_sum}/{n_weighted_sum}")
        print(f"   -> ê³„ì‚°ëœ NSI ì ìˆ˜ (Raw): {nsi_score:.3f}")

        monthly_results.append({
            "date": month_start_date, "nsi_score": nsi_score,
            "summary": final_summary, "analysis_results": all_analysis_results
        })

    # --- ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° í›„ì²˜ë¦¬ ---
    df_monthly = pd.DataFrame(monthly_results)
    if df_monthly.empty: 
        return pd.DataFrame()
    
    df_monthly['nsi_score_smoothed'] = df_monthly['nsi_score'].ewm(span=6, adjust=False).mean()
    print("\nâœ… 5ê°œì›” ì§€ìˆ˜ì´ë™í‰ê· (EWM) ì ìš© ì™„ë£Œ. í‰í™œí™”ëœ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ì§€ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")

    valid_scores = df_monthly['nsi_score_smoothed'].dropna()
    if not valid_scores.empty:
        min_val, max_val = valid_scores.min(), valid_scores.max()
        range_val = max_val - min_val
        if range_val == 0:
            df_monthly['final_supply_demand_index'] = 50.0
        else:
            df_monthly['final_supply_demand_index'] = df_monthly.apply(
                lambda row: ((row['nsi_score_smoothed'] - min_val) / range_val) * 100 if pd.notna(row['nsi_score_smoothed']) else np.nan,
                axis=1
            )
    else:
        df_monthly['final_supply_demand_index'] = np.nan
    df_monthly['final_supply_demand_index'].fillna(50.0, inplace=True)
    
    return df_monthly
